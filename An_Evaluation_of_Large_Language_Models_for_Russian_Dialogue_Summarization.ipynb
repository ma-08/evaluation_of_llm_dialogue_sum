{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnurspNVJS8c"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T24NOc8sJVO2"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM7N1zD-KOWE"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F0kpqR6g51-"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"d0rj/dialogsum-ru\", split=\"test\")\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "df['dialogue_length'] = df['dialogue'].apply(lambda x: len(x.split()))\n",
        "df['summary_length'] = df['summary'].apply(lambda x: len(x.split()))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "dialogue_bins = pd.cut(df['dialogue_length'], bins=range(0, 350, 25))\n",
        "dialogue_counts = dialogue_bins.value_counts().sort_index()\n",
        "ax1.bar(dialogue_counts.index.astype(str), dialogue_counts.values, color='#1f77b4')\n",
        "ax1.set_title('Распределение длины диалогов', fontsize=12)\n",
        "ax1.set_xlabel('Количество слов', fontsize=10)\n",
        "ax1.set_ylabel('Частота', fontsize=10)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "summary_bins = pd.cut(df['summary_length'], bins=range(0, 100, 10))\n",
        "summary_counts = summary_bins.value_counts().sort_index()\n",
        "ax2.bar(summary_counts.index.astype(str), summary_counts.values, color='#ff7f0e')\n",
        "ax2.set_title('Распределение длины саммари', fontsize=12)\n",
        "ax2.set_xlabel('Количество слов', fontsize=10)\n",
        "ax2.set_ylabel('Частота', fontsize=10)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "plt.suptitle('Анализ длины текстов в датасете DialogSum-RU', fontsize=14, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NRmvdwC0_K93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "\n",
        "MODELS = {\n",
        "    \"llama3-8b\": \"meta-llama/Meta-Llama-3-8B\",\n",
        "    \"saiga_llama3_8b\": \"IlyaGusev/saiga_llama3_8b\",\n",
        "    \"fred-t5\": \"RussianNLP/FRED-T5-Summarizer\",\n",
        "    \"falcon-7b\": \"tiiuae/falcon-7b-instruct\"\n",
        "}\n",
        "\n",
        "DATASETS = [\"d0rj/samsum-ru\", \"d0rj/dialogsum-ru\"]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_LENGTH = 1024\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "if not os.path.exists(\"summarization_results\"):\n",
        "    os.makedirs(\"summarization_results\")\n",
        "\n",
        "def load_model(model_name):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODELS[model_name])\n",
        "    if \"t5\" in model_name.lower():\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            MODELS[model_name],\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODELS[model_name],\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def get_prompt_template(model_name, dialog):\n",
        "    if \"t5\" in model_name.lower():\n",
        "        return f\"\"\"<LM>Вы — ассистент для написания краткого содержания диалогов. Вам будет дан диалог, и вы должны создать его краткое содержание. Содержание должно включать всю важную информацию, быть коротким и лаконичным. В нем не должно быть лишней информации или данных, не относящихся к диалогу.\n",
        "Напишите краткое содержание этого диалога:\n",
        "{dialog}\n",
        "Краткое содержание:\"\"\"\n",
        "    elif \"falcon\" in model_name.lower():\n",
        "        return f\"\"\"Вы — ассистент для написания краткого содержания диалогов. Вам будет дан диалог, и вы должны создать его краткое содержание. Содержание должно включать всю важную информацию, быть коротким и лаконичным. В нем не должно быть лишней информации или данных, не относящихся к диалогу.\n",
        "Напишите краткое содержание этого диалога:\n",
        "{dialog}\n",
        "Краткое содержание:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "Вы — ассистент для написания краткого содержания диалогов. Вам будет дан диалог, и вы должны создать его краткое содержание. Содержание должно включать всю важную информацию, быть коротким и лаконичным. В нем не должно быть лишней информации или данных, не относящихся к диалогу.\n",
        "Напишите краткое содержание этого диалога:\n",
        "{dialog}\n",
        "Краткое содержание:\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "def generate_summary(model, tokenizer, model_name, dialog):\n",
        "    prompt = get_prompt_template(model_name, dialog)\n",
        "    if \"t5\" in model_name.lower():\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True).to(DEVICE)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    else:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True).to(DEVICE)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def evaluate_summary(reference, prediction, model_name):\n",
        "    processed_pred = postprocess_summary(prediction, model_name)\n",
        "    rouge_score = rouge.compute(\n",
        "        predictions=[processed_pred],\n",
        "        references=[reference],\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return {\n",
        "        \"rougeL\": rouge_score[\"rougeL\"],\n",
        "        \"rouge1\": rouge_score[\"rouge1\"],\n",
        "        \"rouge2\": rouge_score[\"rouge2\"],\n",
        "        \"processed_summary\": processed_pred\n",
        "    }\n",
        "\n",
        "def postprocess_summary(text, model_name):\n",
        "    remove_self_evaluation = True,\n",
        "    remove_special_tokens = True,\n",
        "    deduplicate_long_texts = True,\n",
        "    deduplication_threshold = 1800\n",
        "    if remove_special_tokens:\n",
        "        text = re.sub(r'<\\|[^>]+\\|>', '', text)\n",
        "    if deduplicate_long_texts and len(text) > deduplication_threshold:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        unique_sentences = []\n",
        "        seen_sentences = set()\n",
        "        for sent in sentences:\n",
        "            normalized = re.sub(r'\\W+', '', sent.lower())\n",
        "            if normalized not in seen_sentences:\n",
        "                seen_sentences.add(normalized)\n",
        "                unique_sentences.append(sent)\n",
        "        text = ' '.join(unique_sentences)\n",
        "\n",
        "    return text\n",
        "\n",
        "def main():\n",
        "    model_results = {model_name: [] for model_name in MODELS.keys()}\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"\\nЗагрузка {model_name}...\")\n",
        "        model, tokenizer = load_model(model_name)\n",
        "        for dataset_name in DATASETS:\n",
        "            print(f\"Обработка датасета {dataset_name}...\")\n",
        "            dataset = load_dataset(dataset_name, split=\"test[:200]\")\n",
        "            for i, example in enumerate(dataset):\n",
        "                dialog = example[\"dialogue\"]\n",
        "                reference = example[\"summary\"]\n",
        "                summary = generate_summary(model, tokenizer, model_name, dialog)\n",
        "                metrics = evaluate_summary(reference, summary, model_name)\n",
        "                model_results[model_name].append({\n",
        "                    \"model\": model_name,\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"rougeL\": metrics[\"rougeL\"],\n",
        "                    \"rouge1\": metrics[\"rouge1\"],\n",
        "                    \"rouge2\": metrics[\"rouge2\"],\n",
        "                    \"summary\": metrics[\"processed_summary\"],\n",
        "                    \"reference\": reference\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(model_results[model_name])\n",
        "        filename = f\"summarization_results/{model_name}_results.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "m8v56aDpXSoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('falcon-7b_summarization_results.csv')\n",
        "df[['rougeL', 'rouge1', 'rouge2']] = df[['rougeL', 'rouge1', 'rouge2']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "first_200 = df.head(200)\n",
        "avg_first_200 = first_200[['rougeL', 'rouge1', 'rouge2']].mean()\n",
        "\n",
        "next_400 = df.iloc[200:400]\n",
        "avg_next_400 = next_400[['rougeL', 'rouge1', 'rouge2']].mean()\n",
        "\n",
        "print(avg_first_200)\n",
        "print(avg_next_400)"
      ],
      "metadata": {
        "id": "Rt6nO8T-Xnrq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}